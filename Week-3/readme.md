## EIP-4 Week-3

## Training Accuracy ~ 84.14%

## Test Accuracy 	   ~ 82.93%

## Baseline Model Logs

```
390/390 [==============================] - 10s 26ms/step - loss: 1.9611 - acc: 0.2466 - val_loss: 1.5215 - val_acc: 0.4347
Epoch 2/50
390/390 [==============================] - 7s 18ms/step - loss: 1.4296 - acc: 0.4775 - val_loss: 1.1909 - val_acc: 0.5578
Epoch 3/50
390/390 [==============================] - 7s 18ms/step - loss: 1.1999 - acc: 0.5697 - val_loss: 1.0086 - val_acc: 0.6366
Epoch 4/50
390/390 [==============================] - 7s 18ms/step - loss: 1.0592 - acc: 0.6270 - val_loss: 0.8966 - val_acc: 0.6893
Epoch 5/50
390/390 [==============================] - 7s 18ms/step - loss: 0.9352 - acc: 0.6750 - val_loss: 0.8339 - val_acc: 0.7113
Epoch 6/50
390/390 [==============================] - 7s 18ms/step - loss: 0.8604 - acc: 0.7037 - val_loss: 0.7637 - val_acc: 0.7350
Epoch 7/50
390/390 [==============================] - 7s 18ms/step - loss: 0.8071 - acc: 0.7223 - val_loss: 0.7241 - val_acc: 0.7524
Epoch 8/50
390/390 [==============================] - 7s 18ms/step - loss: 0.7593 - acc: 0.7422 - val_loss: 0.7037 - val_acc: 0.7610
Epoch 9/50
390/390 [==============================] - 7s 18ms/step - loss: 0.7244 - acc: 0.7505 - val_loss: 0.6604 - val_acc: 0.7738
Epoch 10/50
390/390 [==============================] - 7s 18ms/step - loss: 0.6843 - acc: 0.7672 - val_loss: 0.6334 - val_acc: 0.7835
Epoch 11/50
390/390 [==============================] - 7s 18ms/step - loss: 0.6551 - acc: 0.7767 - val_loss: 0.6628 - val_acc: 0.7741
Epoch 12/50
390/390 [==============================] - 7s 18ms/step - loss: 0.6325 - acc: 0.7853 - val_loss: 0.6239 - val_acc: 0.7841
Epoch 13/50
390/390 [==============================] - 7s 18ms/step - loss: 0.6240 - acc: 0.7890 - val_loss: 0.5980 - val_acc: 0.7957
Epoch 14/50
390/390 [==============================] - 7s 18ms/step - loss: 0.5967 - acc: 0.7966 - val_loss: 0.6128 - val_acc: 0.7931
Epoch 15/50
390/390 [==============================] - 7s 18ms/step - loss: 0.5736 - acc: 0.8041 - val_loss: 0.6014 - val_acc: 0.7969
Epoch 16/50
390/390 [==============================] - 7s 18ms/step - loss: 0.5586 - acc: 0.8078 - val_loss: 0.5774 - val_acc: 0.8050
Epoch 17/50
390/390 [==============================] - 7s 18ms/step - loss: 0.5402 - acc: 0.8159 - val_loss: 0.5757 - val_acc: 0.8046
Epoch 18/50
390/390 [==============================] - 7s 18ms/step - loss: 0.5394 - acc: 0.8164 - val_loss: 0.5808 - val_acc: 0.8069
Epoch 19/50
390/390 [==============================] - 7s 18ms/step - loss: 0.5312 - acc: 0.8206 - val_loss: 0.5708 - val_acc: 0.8134
Epoch 20/50
390/390 [==============================] - 7s 18ms/step - loss: 0.5135 - acc: 0.8232 - val_loss: 0.5670 - val_acc: 0.8113
Epoch 21/50
390/390 [==============================] - 7s 18ms/step - loss: 0.5007 - acc: 0.8299 - val_loss: 0.5767 - val_acc: 0.8078
Epoch 22/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4920 - acc: 0.8329 - val_loss: 0.5891 - val_acc: 0.8065
Epoch 23/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4784 - acc: 0.8378 - val_loss: 0.5996 - val_acc: 0.8071
Epoch 24/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4746 - acc: 0.8372 - val_loss: 0.5595 - val_acc: 0.8142
Epoch 25/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4658 - acc: 0.8423 - val_loss: 0.5681 - val_acc: 0.8131
Epoch 26/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4504 - acc: 0.8470 - val_loss: 0.5805 - val_acc: 0.8147
Epoch 27/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4492 - acc: 0.8480 - val_loss: 0.5863 - val_acc: 0.8114
Epoch 28/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4438 - acc: 0.8500 - val_loss: 0.5885 - val_acc: 0.8149
Epoch 29/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4402 - acc: 0.8511 - val_loss: 0.5621 - val_acc: 0.8178
Epoch 30/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4312 - acc: 0.8531 - val_loss: 0.5567 - val_acc: 0.8192
Epoch 31/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4293 - acc: 0.8525 - val_loss: 0.5654 - val_acc: 0.8190
Epoch 32/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4173 - acc: 0.8604 - val_loss: 0.5705 - val_acc: 0.8172
Epoch 33/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4099 - acc: 0.8612 - val_loss: 0.5904 - val_acc: 0.8099
Epoch 34/50
390/390 [==============================] - 7s 18ms/step - loss: 0.4048 - acc: 0.8633 - val_loss: 0.5760 - val_acc: 0.8207
Epoch 35/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3993 - acc: 0.8652 - val_loss: 0.5774 - val_acc: 0.8221
Epoch 36/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3885 - acc: 0.8694 - val_loss: 0.5581 - val_acc: 0.8169
Epoch 37/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3980 - acc: 0.8651 - val_loss: 0.5620 - val_acc: 0.8235
Epoch 38/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3902 - acc: 0.8687 - val_loss: 0.5716 - val_acc: 0.8211
Epoch 39/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3871 - acc: 0.8682 - val_loss: 0.5538 - val_acc: 0.8282
Epoch 40/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3823 - acc: 0.8705 - val_loss: 0.5636 - val_acc: 0.8191
Epoch 41/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3819 - acc: 0.8735 - val_loss: 0.5775 - val_acc: 0.8211
Epoch 42/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3665 - acc: 0.8754 - val_loss: 0.5781 - val_acc: 0.8199
Epoch 43/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3650 - acc: 0.8781 - val_loss: 0.5952 - val_acc: 0.8188
Epoch 44/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3552 - acc: 0.8789 - val_loss: 0.5714 - val_acc: 0.8252
Epoch 45/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3649 - acc: 0.8778 - val_loss: 0.5933 - val_acc: 0.8223
Epoch 46/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3590 - acc: 0.8781 - val_loss: 0.5747 - val_acc: 0.8249
Epoch 47/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3590 - acc: 0.8795 - val_loss: 0.5658 - val_acc: 0.8250
Epoch 48/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3421 - acc: 0.8845 - val_loss: 0.6013 - val_acc: 0.8208
Epoch 49/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3454 - acc: 0.8853 - val_loss: 0.6044 - val_acc: 0.8145
Epoch 50/50
390/390 [==============================] - 7s 18ms/step - loss: 0.3454 - acc: 0.8860 - val_loss: 0.5617 - val_acc: 0.8265
Model took 354.44 seconds to train
```
## Target Accuracy > 82.5 %

## Model with Depthwise Separable Convolution
```
model1.add(SeparableConv2D(32, 3, padding='same', activation='relu', use_bias=False, input_shape=(32, 32, 3)))  # 32 RF--> 3
model1.add(BatchNormalization())
model1.add(Dropout(0.1))

model1.add(SeparableConv2D(64, 3, padding='same', activation='relu', use_bias=False))                           # 32 RF--> 5
model1.add(BatchNormalization())
model1.add(Dropout(0.1))
model1.add(SeparableConv2D(64, 3, padding='valid', activation='relu', use_bias=False))                          # 30 RF--> 7
model1.add(BatchNormalization())
model1.add(Dropout(0.1))
model1.add(SeparableConv2D(128, 3, padding='same', activation='relu', use_bias=False))                          # 30 RF--> 9
model1.add(BatchNormalization())
model1.add(Dropout(0.1))
model1.add(SeparableConv2D(128, 3, padding='valid', activation='relu', use_bias=False))                         # 28 RF--> 11
model1.add(BatchNormalization())
model1.add(Dropout(0.1)) 

model1.add(SeparableConv2D(32, 1, activation='relu', use_bias=False))                                           # 28 RF--> 11                                      
model1.add(MaxPooling2D())                                                                                      # 14 RF--> 12

model1.add(SeparableConv2D(64, 3, padding='same', activation='relu', use_bias=False))                           # 14 RF--> 16
model1.add(BatchNormalization())
model1.add(Dropout(0.1))
model1.add(SeparableConv2D(128, 3, padding='same', activation='relu', use_bias=False))                          # 14 RF--> 20
model1.add(BatchNormalization())
model1.add(Dropout(0.1))

model1.add(SeparableConv2D(32, 1, activation='relu', use_bias=False))                                           # 14 RF--> 20                                      
model1.add(MaxPooling2D())                                                                                      # 7  RF--> 22

model1.add(SeparableConv2D(32, 3, padding='same', activation='relu', use_bias=False))                           # 7  RF--> 26
model1.add(BatchNormalization())
model1.add(Dropout(0.1))
model1.add(SeparableConv2D(64, 3, padding='valid', activation='relu', use_bias=False))                          # 5  RF--> 34
model1.add(BatchNormalization())
model1.add(Dropout(0.1))
model1.add(SeparableConv2D(64, 3, padding='same', activation='relu', use_bias=False))                           # 5  RF--> 42
model1.add(BatchNormalization())
model1.add(Dropout(0.1))
model1.add(SeparableConv2D(128, 3, padding='valid', activation='relu', use_bias=False))                         # 3  RF--> 50
model1.add(BatchNormalization())
model1.add(Dropout(0.1))

model1.add(SeparableConv2D(10, 1, activation='relu', use_bias=False))                                           # 3  RF--> 50
model1.add(SeparableConv2D(10, 3, padding='valid', activation='relu', use_bias=False))                          # 1  RF--> 58
model1.add(Flatten())
model1.add(Activation('softmax'))
```

## Summary

```
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
separable_conv2d_1 (Separabl (None, 32, 32, 32)        123       
_________________________________________________________________
batch_normalization_1 (Batch (None, 32, 32, 32)        128       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32, 32, 32)        0         
_________________________________________________________________
separable_conv2d_2 (Separabl (None, 32, 32, 64)        2336      
_________________________________________________________________
batch_normalization_2 (Batch (None, 32, 32, 64)        256       
_________________________________________________________________
dropout_2 (Dropout)          (None, 32, 32, 64)        0         
_________________________________________________________________
separable_conv2d_3 (Separabl (None, 30, 30, 64)        4672      
_________________________________________________________________
batch_normalization_3 (Batch (None, 30, 30, 64)        256       
_________________________________________________________________
dropout_3 (Dropout)          (None, 30, 30, 64)        0         
_________________________________________________________________
separable_conv2d_4 (Separabl (None, 30, 30, 64)        4672      
_________________________________________________________________
batch_normalization_4 (Batch (None, 30, 30, 64)        256       
_________________________________________________________________
dropout_4 (Dropout)          (None, 30, 30, 64)        0         
_________________________________________________________________
separable_conv2d_5 (Separabl (None, 28, 28, 128)       8768      
_________________________________________________________________
batch_normalization_5 (Batch (None, 28, 28, 128)       512       
_________________________________________________________________
dropout_5 (Dropout)          (None, 28, 28, 128)       0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 28, 28, 32)        4096      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         
_________________________________________________________________
separable_conv2d_6 (Separabl (None, 14, 14, 64)        2336      
_________________________________________________________________
batch_normalization_6 (Batch (None, 14, 14, 64)        256       
_________________________________________________________________
dropout_6 (Dropout)          (None, 14, 14, 64)        0         
_________________________________________________________________
separable_conv2d_7 (Separabl (None, 14, 14, 128)       8768      
_________________________________________________________________
batch_normalization_7 (Batch (None, 14, 14, 128)       512       
_________________________________________________________________
dropout_7 (Dropout)          (None, 14, 14, 128)       0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 14, 14, 32)        4096      
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         
_________________________________________________________________
separable_conv2d_8 (Separabl (None, 7, 7, 32)          1312      
_________________________________________________________________
batch_normalization_8 (Batch (None, 7, 7, 32)          128       
_________________________________________________________________
dropout_8 (Dropout)          (None, 7, 7, 32)          0         
_________________________________________________________________
separable_conv2d_9 (Separabl (None, 5, 5, 64)          2336      
_________________________________________________________________
batch_normalization_9 (Batch (None, 5, 5, 64)          256       
_________________________________________________________________
dropout_9 (Dropout)          (None, 5, 5, 64)          0         
_________________________________________________________________
separable_conv2d_10 (Separab (None, 5, 5, 64)          4672      
_________________________________________________________________
batch_normalization_10 (Batc (None, 5, 5, 64)          256       
_________________________________________________________________
dropout_10 (Dropout)         (None, 5, 5, 64)          0         
_________________________________________________________________
separable_conv2d_11 (Separab (None, 3, 3, 128)         8768      
_________________________________________________________________
batch_normalization_11 (Batc (None, 3, 3, 128)         512       
_________________________________________________________________
dropout_11 (Dropout)         (None, 3, 3, 128)         0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 3, 3, 10)          1280      
_________________________________________________________________
separable_conv2d_12 (Separab (None, 1, 1, 10)          190       
_________________________________________________________________
flatten_1 (Flatten)          (None, 10)                0         
_________________________________________________________________
activation_1 (Activation)    (None, 10)                0         
=================================================================
Total params: 61,753
Trainable params: 60,089
Non-trainable params: 1,664
_________________________________________________________________
```


## Logs
```
Epoch 1/50

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
390/390 [==============================] - 33s 84ms/step - loss: 1.7711 - acc: 0.3619 - val_loss: 2.6390 - val_acc: 0.3782

Epoch 00001: val_acc improved from -inf to 0.37820, saving model to /content/best_weights.hdf5
Epoch 2/50

Epoch 00002: LearningRateScheduler setting learning rate to 0.0026809651.
390/390 [==============================] - 28s 72ms/step - loss: 1.2772 - acc: 0.5385 - val_loss: 1.4015 - val_acc: 0.5469

Epoch 00002: val_acc improved from 0.37820 to 0.54690, saving model to /content/best_weights.hdf5
Epoch 3/50

Epoch 00003: LearningRateScheduler setting learning rate to 0.0024232633.
390/390 [==============================] - 28s 72ms/step - loss: 1.1109 - acc: 0.6048 - val_loss: 1.1680 - val_acc: 0.6058

Epoch 00003: val_acc improved from 0.54690 to 0.60580, saving model to /content/best_weights.hdf5
Epoch 4/50

Epoch 00004: LearningRateScheduler setting learning rate to 0.002210759.
390/390 [==============================] - 28s 72ms/step - loss: 1.0033 - acc: 0.6442 - val_loss: 1.0551 - val_acc: 0.6454

Epoch 00004: val_acc improved from 0.60580 to 0.64540, saving model to /content/best_weights.hdf5
Epoch 5/50

Epoch 00005: LearningRateScheduler setting learning rate to 0.0020325203.
390/390 [==============================] - 28s 72ms/step - loss: 0.9325 - acc: 0.6676 - val_loss: 1.0670 - val_acc: 0.6442

Epoch 00005: val_acc did not improve from 0.64540
Epoch 6/50

Epoch 00006: LearningRateScheduler setting learning rate to 0.0018808777.
390/390 [==============================] - 28s 72ms/step - loss: 0.8725 - acc: 0.6903 - val_loss: 0.9000 - val_acc: 0.6901

Epoch 00006: val_acc improved from 0.64540 to 0.69010, saving model to /content/best_weights.hdf5
Epoch 7/50

Epoch 00007: LearningRateScheduler setting learning rate to 0.0017502917.
390/390 [==============================] - 28s 73ms/step - loss: 0.8260 - acc: 0.7082 - val_loss: 0.8525 - val_acc: 0.7155

Epoch 00007: val_acc improved from 0.69010 to 0.71550, saving model to /content/best_weights.hdf5
Epoch 8/50

Epoch 00008: LearningRateScheduler setting learning rate to 0.0016366612.
390/390 [==============================] - 28s 72ms/step - loss: 0.7868 - acc: 0.7228 - val_loss: 0.8606 - val_acc: 0.7079

Epoch 00008: val_acc did not improve from 0.71550
Epoch 9/50

Epoch 00009: LearningRateScheduler setting learning rate to 0.0015368852.
390/390 [==============================] - 28s 72ms/step - loss: 0.7551 - acc: 0.7334 - val_loss: 0.9850 - val_acc: 0.6733

Epoch 00009: val_acc did not improve from 0.71550
Epoch 10/50

Epoch 00010: LearningRateScheduler setting learning rate to 0.0014485756.
390/390 [==============================] - 28s 73ms/step - loss: 0.7309 - acc: 0.7442 - val_loss: 0.7861 - val_acc: 0.7363

Epoch 00010: val_acc improved from 0.71550 to 0.73630, saving model to /content/best_weights.hdf5
Epoch 11/50

Epoch 00011: LearningRateScheduler setting learning rate to 0.001369863.
390/390 [==============================] - 29s 73ms/step - loss: 0.7049 - acc: 0.7527 - val_loss: 0.7420 - val_acc: 0.7468

Epoch 00011: val_acc improved from 0.73630 to 0.74680, saving model to /content/best_weights.hdf5
Epoch 12/50

Epoch 00012: LearningRateScheduler setting learning rate to 0.0012992638.
390/390 [==============================] - 28s 73ms/step - loss: 0.6933 - acc: 0.7557 - val_loss: 0.7724 - val_acc: 0.7349

Epoch 00012: val_acc did not improve from 0.74680
Epoch 13/50

Epoch 00013: LearningRateScheduler setting learning rate to 0.0012355848.
390/390 [==============================] - 28s 72ms/step - loss: 0.6653 - acc: 0.7656 - val_loss: 0.8016 - val_acc: 0.7342

Epoch 00013: val_acc did not improve from 0.74680
Epoch 14/50

Epoch 00014: LearningRateScheduler setting learning rate to 0.0011778563.
390/390 [==============================] - 28s 72ms/step - loss: 0.6583 - acc: 0.7685 - val_loss: 0.7165 - val_acc: 0.7573

Epoch 00014: val_acc improved from 0.74680 to 0.75730, saving model to /content/best_weights.hdf5
Epoch 15/50

Epoch 00015: LearningRateScheduler setting learning rate to 0.0011252813.
390/390 [==============================] - 28s 71ms/step - loss: 0.6444 - acc: 0.7726 - val_loss: 0.6813 - val_acc: 0.7695

Epoch 00015: val_acc improved from 0.75730 to 0.76950, saving model to /content/best_weights.hdf5
Epoch 16/50

Epoch 00016: LearningRateScheduler setting learning rate to 0.0010771993.
390/390 [==============================] - 28s 72ms/step - loss: 0.6312 - acc: 0.7787 - val_loss: 0.6563 - val_acc: 0.7791

Epoch 00016: val_acc improved from 0.76950 to 0.77910, saving model to /content/best_weights.hdf5
Epoch 17/50

Epoch 00017: LearningRateScheduler setting learning rate to 0.0010330579.
390/390 [==============================] - 28s 72ms/step - loss: 0.6166 - acc: 0.7833 - val_loss: 0.6506 - val_acc: 0.7830

Epoch 00017: val_acc improved from 0.77910 to 0.78300, saving model to /content/best_weights.hdf5
Epoch 18/50

Epoch 00018: LearningRateScheduler setting learning rate to 0.0009923917.
390/390 [==============================] - 28s 71ms/step - loss: 0.6049 - acc: 0.7889 - val_loss: 0.6229 - val_acc: 0.7880

Epoch 00018: val_acc improved from 0.78300 to 0.78800, saving model to /content/best_weights.hdf5
Epoch 19/50

Epoch 00019: LearningRateScheduler setting learning rate to 0.0009548059.
390/390 [==============================] - 28s 72ms/step - loss: 0.5940 - acc: 0.7923 - val_loss: 0.6540 - val_acc: 0.7815

Epoch 00019: val_acc did not improve from 0.78800
Epoch 20/50

Epoch 00020: LearningRateScheduler setting learning rate to 0.0009199632.
390/390 [==============================] - 28s 71ms/step - loss: 0.5882 - acc: 0.7960 - val_loss: 0.6223 - val_acc: 0.7878

Epoch 00020: val_acc did not improve from 0.78800
Epoch 21/50

Epoch 00021: LearningRateScheduler setting learning rate to 0.000887574.
390/390 [==============================] - 28s 71ms/step - loss: 0.5785 - acc: 0.7976 - val_loss: 0.7078 - val_acc: 0.7657

Epoch 00021: val_acc did not improve from 0.78800
Epoch 22/50

Epoch 00022: LearningRateScheduler setting learning rate to 0.0008573878.
390/390 [==============================] - 28s 72ms/step - loss: 0.5700 - acc: 0.8003 - val_loss: 0.6063 - val_acc: 0.7939

Epoch 00022: val_acc improved from 0.78800 to 0.79390, saving model to /content/best_weights.hdf5
Epoch 23/50

Epoch 00023: LearningRateScheduler setting learning rate to 0.0008291874.
390/390 [==============================] - 28s 72ms/step - loss: 0.5650 - acc: 0.8026 - val_loss: 0.6238 - val_acc: 0.7888

Epoch 00023: val_acc did not improve from 0.79390
Epoch 24/50

Epoch 00024: LearningRateScheduler setting learning rate to 0.000802783.
390/390 [==============================] - 28s 72ms/step - loss: 0.5570 - acc: 0.8059 - val_loss: 0.6018 - val_acc: 0.7981

Epoch 00024: val_acc improved from 0.79390 to 0.79810, saving model to /content/best_weights.hdf5
Epoch 25/50

Epoch 00025: LearningRateScheduler setting learning rate to 0.0007780083.
390/390 [==============================] - 28s 72ms/step - loss: 0.5460 - acc: 0.8093 - val_loss: 0.6236 - val_acc: 0.7886

Epoch 00025: val_acc did not improve from 0.79810
Epoch 26/50

Epoch 00026: LearningRateScheduler setting learning rate to 0.000754717.
390/390 [==============================] - 28s 71ms/step - loss: 0.5461 - acc: 0.8081 - val_loss: 0.6105 - val_acc: 0.7985

Epoch 00026: val_acc improved from 0.79810 to 0.79850, saving model to /content/best_weights.hdf5
Epoch 27/50

Epoch 00027: LearningRateScheduler setting learning rate to 0.0007327797.
390/390 [==============================] - 28s 71ms/step - loss: 0.5385 - acc: 0.8144 - val_loss: 0.5842 - val_acc: 0.8056

Epoch 00027: val_acc improved from 0.79850 to 0.80560, saving model to /content/best_weights.hdf5
Epoch 28/50

Epoch 00028: LearningRateScheduler setting learning rate to 0.0007120817.
390/390 [==============================] - 28s 71ms/step - loss: 0.5309 - acc: 0.8139 - val_loss: 0.5563 - val_acc: 0.8095

Epoch 00028: val_acc improved from 0.80560 to 0.80950, saving model to /content/best_weights.hdf5
Epoch 29/50

Epoch 00029: LearningRateScheduler setting learning rate to 0.0006925208.
390/390 [==============================] - 28s 72ms/step - loss: 0.5285 - acc: 0.8145 - val_loss: 0.6054 - val_acc: 0.7976

Epoch 00029: val_acc did not improve from 0.80950
Epoch 30/50

Epoch 00030: LearningRateScheduler setting learning rate to 0.0006740058.
390/390 [==============================] - 28s 71ms/step - loss: 0.5248 - acc: 0.8162 - val_loss: 0.5856 - val_acc: 0.8053

Epoch 00030: val_acc did not improve from 0.80950
Epoch 31/50

Epoch 00031: LearningRateScheduler setting learning rate to 0.0006564551.
390/390 [==============================] - 28s 72ms/step - loss: 0.5177 - acc: 0.8202 - val_loss: 0.5719 - val_acc: 0.8069

Epoch 00031: val_acc did not improve from 0.80950
Epoch 32/50

Epoch 00032: LearningRateScheduler setting learning rate to 0.0006397953.
390/390 [==============================] - 28s 71ms/step - loss: 0.5141 - acc: 0.8200 - val_loss: 0.5414 - val_acc: 0.8160

Epoch 00032: val_acc improved from 0.80950 to 0.81600, saving model to /content/best_weights.hdf5
Epoch 33/50

Epoch 00033: LearningRateScheduler setting learning rate to 0.0006239601.
390/390 [==============================] - 28s 72ms/step - loss: 0.5111 - acc: 0.8215 - val_loss: 0.5603 - val_acc: 0.8137

Epoch 00033: val_acc did not improve from 0.81600
Epoch 34/50

Epoch 00034: LearningRateScheduler setting learning rate to 0.0006088898.
390/390 [==============================] - 28s 71ms/step - loss: 0.5064 - acc: 0.8218 - val_loss: 0.5567 - val_acc: 0.8109

Epoch 00034: val_acc did not improve from 0.81600
Epoch 35/50

Epoch 00035: LearningRateScheduler setting learning rate to 0.0005945303.
390/390 [==============================] - 28s 71ms/step - loss: 0.4992 - acc: 0.8262 - val_loss: 0.5576 - val_acc: 0.8106

Epoch 00035: val_acc did not improve from 0.81600
Epoch 36/50

Epoch 00036: LearningRateScheduler setting learning rate to 0.0005808325.
390/390 [==============================] - 28s 72ms/step - loss: 0.5012 - acc: 0.8243 - val_loss: 0.5722 - val_acc: 0.8116

Epoch 00036: val_acc did not improve from 0.81600
Epoch 37/50

Epoch 00037: LearningRateScheduler setting learning rate to 0.0005677517.
390/390 [==============================] - 28s 71ms/step - loss: 0.4901 - acc: 0.8290 - val_loss: 0.5448 - val_acc: 0.8182

Epoch 00037: val_acc improved from 0.81600 to 0.81820, saving model to /content/best_weights.hdf5
Epoch 38/50

Epoch 00038: LearningRateScheduler setting learning rate to 0.0005552471.
390/390 [==============================] - 28s 71ms/step - loss: 0.4875 - acc: 0.8303 - val_loss: 0.5470 - val_acc: 0.8207

Epoch 00038: val_acc improved from 0.81820 to 0.82070, saving model to /content/best_weights.hdf5
Epoch 39/50

Epoch 00039: LearningRateScheduler setting learning rate to 0.0005432814.
390/390 [==============================] - 28s 71ms/step - loss: 0.4872 - acc: 0.8300 - val_loss: 0.5670 - val_acc: 0.8126

Epoch 00039: val_acc did not improve from 0.82070
Epoch 40/50

Epoch 00040: LearningRateScheduler setting learning rate to 0.0005318206.
390/390 [==============================] - 28s 71ms/step - loss: 0.4861 - acc: 0.8308 - val_loss: 0.5529 - val_acc: 0.8164

Epoch 00040: val_acc did not improve from 0.82070
Epoch 41/50

Epoch 00041: LearningRateScheduler setting learning rate to 0.0005208333.
390/390 [==============================] - 28s 71ms/step - loss: 0.4791 - acc: 0.8331 - val_loss: 0.5030 - val_acc: 0.8293

Epoch 00041: val_acc improved from 0.82070 to 0.82930, saving model to /content/best_weights.hdf5
Epoch 42/50

Epoch 00042: LearningRateScheduler setting learning rate to 0.0005102909.
390/390 [==============================] - 28s 71ms/step - loss: 0.4751 - acc: 0.8325 - val_loss: 0.5556 - val_acc: 0.8158

Epoch 00042: val_acc did not improve from 0.82930
Epoch 43/50

Epoch 00043: LearningRateScheduler setting learning rate to 0.0005001667.
390/390 [==============================] - 28s 71ms/step - loss: 0.4765 - acc: 0.8348 - val_loss: 0.5321 - val_acc: 0.8224

Epoch 00043: val_acc did not improve from 0.82930
Epoch 44/50

Epoch 00044: LearningRateScheduler setting learning rate to 0.0004904365.
390/390 [==============================] - 28s 71ms/step - loss: 0.4672 - acc: 0.8361 - val_loss: 0.5150 - val_acc: 0.8261

Epoch 00044: val_acc did not improve from 0.82930
Epoch 45/50

Epoch 00045: LearningRateScheduler setting learning rate to 0.0004810776.
390/390 [==============================] - 28s 71ms/step - loss: 0.4723 - acc: 0.8352 - val_loss: 0.5532 - val_acc: 0.8162

Epoch 00045: val_acc did not improve from 0.82930
Epoch 46/50

Epoch 00046: LearningRateScheduler setting learning rate to 0.0004720692.
390/390 [==============================] - 28s 71ms/step - loss: 0.4722 - acc: 0.8349 - val_loss: 0.5495 - val_acc: 0.8163

Epoch 00046: val_acc did not improve from 0.82930
Epoch 47/50

Epoch 00047: LearningRateScheduler setting learning rate to 0.000463392.
390/390 [==============================] - 28s 71ms/step - loss: 0.4581 - acc: 0.8397 - val_loss: 0.5144 - val_acc: 0.8251

Epoch 00047: val_acc did not improve from 0.82930
Epoch 48/50

Epoch 00048: LearningRateScheduler setting learning rate to 0.0004550281.
390/390 [==============================] - 28s 71ms/step - loss: 0.4604 - acc: 0.8400 - val_loss: 0.5473 - val_acc: 0.8160

Epoch 00048: val_acc did not improve from 0.82930
Epoch 49/50

Epoch 00049: LearningRateScheduler setting learning rate to 0.0004469607.
390/390 [==============================] - 28s 71ms/step - loss: 0.4566 - acc: 0.8414 - val_loss: 0.5188 - val_acc: 0.8255

Epoch 00049: val_acc did not improve from 0.82930
Epoch 50/50

Epoch 00050: LearningRateScheduler setting learning rate to 0.0004391744.
390/390 [==============================] - 28s 71ms/step - loss: 0.4542 - acc: 0.8407 - val_loss: 0.5333 - val_acc: 0.8221

Epoch 00050: val_acc did not improve from 0.82930
```
