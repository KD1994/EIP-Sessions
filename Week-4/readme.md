## EIP-4 Week-3

## Training Accuracy ~ 91.63%

## Test Accuracy 	   ~ 88.48%

## Baseline Model Logs

```
Epoch 1/50
390/390 [==============================] - 77s 196ms/step - loss: 2.8290 - acc: 0.3282 - val_loss: 2.6819 - val_acc: 0.2638

Epoch 00001: val_acc improved from -inf to 0.26380, saving model to /content/best_weights.hdf5
Epoch 2/50
390/390 [==============================] - 68s 176ms/step - loss: 1.9195 - acc: 0.4461 - val_loss: 1.9405 - val_acc: 0.4438

Epoch 00002: val_acc improved from 0.26380 to 0.44380, saving model to /content/best_weights.hdf5
Epoch 3/50
390/390 [==============================] - 68s 175ms/step - loss: 1.6883 - acc: 0.5063 - val_loss: 1.6572 - val_acc: 0.5069

Epoch 00003: val_acc improved from 0.44380 to 0.50690, saving model to /content/best_weights.hdf5
Epoch 4/50
390/390 [==============================] - 68s 175ms/step - loss: 1.5490 - acc: 0.5579 - val_loss: 1.7222 - val_acc: 0.5092

Epoch 00004: val_acc improved from 0.50690 to 0.50920, saving model to /content/best_weights.hdf5
Epoch 5/50
390/390 [==============================] - 68s 175ms/step - loss: 1.4480 - acc: 0.5969 - val_loss: 1.9544 - val_acc: 0.4795

Epoch 00005: val_acc did not improve from 0.50920
Epoch 6/50
390/390 [==============================] - 68s 175ms/step - loss: 1.3725 - acc: 0.6266 - val_loss: 1.7343 - val_acc: 0.5486

Epoch 00006: val_acc improved from 0.50920 to 0.54860, saving model to /content/best_weights.hdf5

Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0009486833062967954.
Epoch 7/50
390/390 [==============================] - 68s 175ms/step - loss: 1.1171 - acc: 0.7023 - val_loss: 1.0647 - val_acc: 0.7173

Epoch 00007: val_acc improved from 0.54860 to 0.71730, saving model to /content/best_weights.hdf5
Epoch 8/50
390/390 [==============================] - 68s 175ms/step - loss: 1.0405 - acc: 0.7235 - val_loss: 1.0120 - val_acc: 0.7265

Epoch 00008: val_acc improved from 0.71730 to 0.72650, saving model to /content/best_weights.hdf5
Epoch 9/50
390/390 [==============================] - 68s 175ms/step - loss: 0.9971 - acc: 0.7363 - val_loss: 0.9875 - val_acc: 0.7446

Epoch 00009: val_acc improved from 0.72650 to 0.74460, saving model to /content/best_weights.hdf5
Epoch 10/50
390/390 [==============================] - 69s 177ms/step - loss: 0.9573 - acc: 0.7470 - val_loss: 1.0275 - val_acc: 0.7413

Epoch 00010: val_acc did not improve from 0.74460
Epoch 11/50
390/390 [==============================] - 69s 176ms/step - loss: 0.9232 - acc: 0.7600 - val_loss: 1.0137 - val_acc: 0.7368

Epoch 00011: val_acc did not improve from 0.74460
Epoch 12/50
390/390 [==============================] - 68s 175ms/step - loss: 0.9007 - acc: 0.7667 - val_loss: 0.9470 - val_acc: 0.7591

Epoch 00012: val_acc improved from 0.74460 to 0.75910, saving model to /content/best_weights.hdf5
Epoch 13/50
390/390 [==============================] - 68s 175ms/step - loss: 0.8704 - acc: 0.7780 - val_loss: 1.0512 - val_acc: 0.7325

Epoch 00013: val_acc did not improve from 0.75910
Epoch 14/50
390/390 [==============================] - 68s 175ms/step - loss: 0.8537 - acc: 0.7805 - val_loss: 1.0273 - val_acc: 0.7403

Epoch 00014: val_acc did not improve from 0.75910
Epoch 15/50
390/390 [==============================] - 68s 175ms/step - loss: 0.8306 - acc: 0.7898 - val_loss: 0.7868 - val_acc: 0.8081

Epoch 00015: val_acc improved from 0.75910 to 0.80810, saving model to /content/best_weights.hdf5
Epoch 16/50
390/390 [==============================] - 68s 175ms/step - loss: 0.8110 - acc: 0.7952 - val_loss: 0.8606 - val_acc: 0.7894

Epoch 00016: val_acc did not improve from 0.80810
Epoch 17/50
390/390 [==============================] - 68s 175ms/step - loss: 0.7956 - acc: 0.8004 - val_loss: 0.8873 - val_acc: 0.7796

Epoch 00017: val_acc did not improve from 0.80810
Epoch 18/50
390/390 [==============================] - 68s 175ms/step - loss: 0.7776 - acc: 0.8056 - val_loss: 0.8660 - val_acc: 0.7868

Epoch 00018: val_acc did not improve from 0.80810

Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0003000000007813074.
Epoch 19/50
390/390 [==============================] - 69s 176ms/step - loss: 0.6827 - acc: 0.8363 - val_loss: 0.6911 - val_acc: 0.8418

Epoch 00019: val_acc improved from 0.80810 to 0.84180, saving model to /content/best_weights.hdf5
Epoch 20/50
390/390 [==============================] - 69s 176ms/step - loss: 0.6392 - acc: 0.8481 - val_loss: 0.6499 - val_acc: 0.8500

Epoch 00020: val_acc improved from 0.84180 to 0.85000, saving model to /content/best_weights.hdf5
Epoch 21/50
390/390 [==============================] - 69s 176ms/step - loss: 0.6220 - acc: 0.8528 - val_loss: 0.6148 - val_acc: 0.8597

Epoch 00021: val_acc improved from 0.85000 to 0.85970, saving model to /content/best_weights.hdf5
Epoch 22/50
390/390 [==============================] - 68s 175ms/step - loss: 0.6045 - acc: 0.8571 - val_loss: 0.6358 - val_acc: 0.8557

Epoch 00022: val_acc did not improve from 0.85970
Epoch 23/50
390/390 [==============================] - 68s 176ms/step - loss: 0.5947 - acc: 0.8599 - val_loss: 0.5922 - val_acc: 0.8647

Epoch 00023: val_acc improved from 0.85970 to 0.86470, saving model to /content/best_weights.hdf5
Epoch 24/50
390/390 [==============================] - 68s 176ms/step - loss: 0.5794 - acc: 0.8639 - val_loss: 0.6311 - val_acc: 0.8536

Epoch 00024: val_acc did not improve from 0.86470
Epoch 25/50
390/390 [==============================] - 69s 176ms/step - loss: 0.5719 - acc: 0.8650 - val_loss: 0.6538 - val_acc: 0.8497

Epoch 00025: val_acc did not improve from 0.86470
Epoch 26/50
390/390 [==============================] - 68s 176ms/step - loss: 0.5614 - acc: 0.8677 - val_loss: 0.5801 - val_acc: 0.8707

Epoch 00026: val_acc improved from 0.86470 to 0.87070, saving model to /content/best_weights.hdf5
Epoch 27/50
390/390 [==============================] - 69s 176ms/step - loss: 0.5490 - acc: 0.8724 - val_loss: 0.6875 - val_acc: 0.8375

Epoch 00027: val_acc did not improve from 0.87070
Epoch 28/50
390/390 [==============================] - 69s 176ms/step - loss: 0.5424 - acc: 0.8746 - val_loss: 0.5845 - val_acc: 0.8625

Epoch 00028: val_acc did not improve from 0.87070
Epoch 29/50
390/390 [==============================] - 68s 176ms/step - loss: 0.5341 - acc: 0.8739 - val_loss: 0.6362 - val_acc: 0.8547

Epoch 00029: val_acc did not improve from 0.87070

Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0001.
Epoch 30/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4889 - acc: 0.8907 - val_loss: 0.5519 - val_acc: 0.8762

Epoch 00030: val_acc improved from 0.87070 to 0.87620, saving model to /content/best_weights.hdf5
Epoch 31/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4810 - acc: 0.8926 - val_loss: 0.5352 - val_acc: 0.8811

Epoch 00031: val_acc improved from 0.87620 to 0.88110, saving model to /content/best_weights.hdf5
Epoch 32/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4708 - acc: 0.8965 - val_loss: 0.5538 - val_acc: 0.8730

Epoch 00032: val_acc did not improve from 0.88110
Epoch 33/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4645 - acc: 0.8979 - val_loss: 0.5375 - val_acc: 0.8819

Epoch 00033: val_acc improved from 0.88110 to 0.88190, saving model to /content/best_weights.hdf5
Epoch 34/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4548 - acc: 0.9007 - val_loss: 0.5453 - val_acc: 0.8776

Epoch 00034: val_acc did not improve from 0.88190
Epoch 35/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4458 - acc: 0.9042 - val_loss: 0.5498 - val_acc: 0.8788

Epoch 00035: val_acc did not improve from 0.88190
Epoch 36/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4468 - acc: 0.9014 - val_loss: 0.5444 - val_acc: 0.8802

Epoch 00036: val_acc did not improve from 0.88190
Epoch 37/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4371 - acc: 0.9044 - val_loss: 0.5522 - val_acc: 0.8756

Epoch 00037: val_acc did not improve from 0.88190
Epoch 38/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4398 - acc: 0.9044 - val_loss: 0.5452 - val_acc: 0.8793

Epoch 00038: val_acc did not improve from 0.88190
Epoch 39/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4318 - acc: 0.9058 - val_loss: 0.5305 - val_acc: 0.8812

Epoch 00039: val_acc did not improve from 0.88190
Epoch 40/50
390/390 [==============================] - 69s 178ms/step - loss: 0.4299 - acc: 0.9059 - val_loss: 0.5279 - val_acc: 0.8842

Epoch 00040: val_acc improved from 0.88190 to 0.88420, saving model to /content/best_weights.hdf5
Epoch 41/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4198 - acc: 0.9109 - val_loss: 0.5263 - val_acc: 0.8848

Epoch 00041: val_acc improved from 0.88420 to 0.88480, saving model to /content/best_weights.hdf5
Epoch 42/50
390/390 [==============================] - 69s 177ms/step - loss: 0.4231 - acc: 0.9084 - val_loss: 0.5270 - val_acc: 0.8844

Epoch 00042: val_acc did not improve from 0.88480
Epoch 43/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4130 - acc: 0.9115 - val_loss: 0.5305 - val_acc: 0.8844

Epoch 00043: val_acc did not improve from 0.88480
Epoch 44/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4126 - acc: 0.9127 - val_loss: 0.5467 - val_acc: 0.8814

Epoch 00044: val_acc did not improve from 0.88480
Epoch 45/50
390/390 [==============================] - 69s 176ms/step - loss: 0.4072 - acc: 0.9133 - val_loss: 0.5547 - val_acc: 0.8784

Epoch 00045: val_acc did not improve from 0.88480
Epoch 46/50
390/390 [==============================] - 68s 175ms/step - loss: 0.4052 - acc: 0.9143 - val_loss: 0.5309 - val_acc: 0.8837

Epoch 00046: val_acc did not improve from 0.88480
Epoch 47/50
390/390 [==============================] - 68s 175ms/step - loss: 0.3982 - acc: 0.9164 - val_loss: 0.5339 - val_acc: 0.8848

Epoch 00047: val_acc did not improve from 0.88480
Epoch 48/50
390/390 [==============================] - 68s 176ms/step - loss: 0.3964 - acc: 0.9166 - val_loss: 0.5454 - val_acc: 0.8804

Epoch 00048: val_acc did not improve from 0.88480
Epoch 49/50
390/390 [==============================] - 68s 175ms/step - loss: 0.3968 - acc: 0.9163 - val_loss: 0.5390 - val_acc: 0.8841

Epoch 00049: val_acc did not improve from 0.88480
Epoch 50/50
390/390 [==============================] - 68s 175ms/step - loss: 0.3934 - acc: 0.9162 - val_loss: 0.5402 - val_acc: 0.8843

Epoch 00050: val_acc did not improve from 0.88480
Model took 3445.20 seconds to train
```


## Summary

```
Model: "model_1"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 32, 32, 3)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 16, 16, 64)   9472        input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 16, 16, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 16, 16, 64)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 16, 16, 64)   36928       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 16, 16, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 16, 16, 64)   0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 16, 16, 64)   36928       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 16, 16, 64)   0           activation_1[0][0]               
                                                                 batch_normalization_3[0][0]      
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 16, 16, 64)   0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 16, 16, 64)   36928       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 16, 16, 64)   256         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 16, 16, 64)   0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 16, 16, 64)   36928       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 16, 16, 64)   256         conv2d_5[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 16, 16, 64)   0           activation_3[0][0]               
                                                                 batch_normalization_5[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 16, 16, 64)   0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 8, 8, 128)    73856       activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 8, 8, 128)    512         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 8, 8, 128)    0           batch_normalization_6[0][0]      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 8, 8, 128)    147584      activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 8, 8, 128)    8320        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 8, 8, 128)    512         conv2d_7[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 8, 8, 128)    0           conv2d_8[0][0]                   
                                                                 batch_normalization_7[0][0]      
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 8, 8, 128)    0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 8, 8, 128)    147584      activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 8, 8, 128)    512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 8, 8, 128)    0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 8, 8, 128)    147584      activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 8, 8, 128)    512         conv2d_10[0][0]                  
__________________________________________________________________________________________________
add_4 (Add)                     (None, 8, 8, 128)    0           activation_7[0][0]               
                                                                 batch_normalization_9[0][0]      
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 8, 8, 128)    0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 4, 4, 256)    295168      activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 4, 4, 256)    1024        conv2d_11[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 4, 4, 256)    0           batch_normalization_10[0][0]     
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 4, 4, 256)    590080      activation_10[0][0]              
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 4, 4, 256)    33024       activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 4, 4, 256)    1024        conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 4, 4, 256)    0           conv2d_13[0][0]                  
                                                                 batch_normalization_11[0][0]     
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 4, 4, 256)    0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 4, 4, 256)    590080      activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 4, 4, 256)    1024        conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 4, 4, 256)    0           batch_normalization_12[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 4, 4, 256)    590080      activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 4, 4, 256)    1024        conv2d_15[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 4, 4, 256)    0           activation_11[0][0]              
                                                                 batch_normalization_13[0][0]     
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 4, 4, 256)    0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 2, 2, 512)    1180160     activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 2, 2, 512)    2048        conv2d_16[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 2, 2, 512)    0           batch_normalization_14[0][0]     
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 2, 2, 512)    2359808     activation_14[0][0]              
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 2, 2, 512)    131584      activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 2, 2, 512)    2048        conv2d_17[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 2, 2, 512)    0           conv2d_18[0][0]                  
                                                                 batch_normalization_15[0][0]     
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 2, 2, 512)    0           add_7[0][0]                      
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 2, 2, 512)    2359808     activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 2, 2, 512)    2048        conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 2, 2, 512)    0           batch_normalization_16[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 2, 2, 512)    2359808     activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 2, 2, 512)    2048        conv2d_20[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 2, 2, 512)    0           activation_15[0][0]              
                                                                 batch_normalization_17[0][0]     
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 2, 2, 512)    0           add_8[0][0]                      
__________________________________________________________________________________________________
average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_17[0][0]              
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 10)           5130        flatten_1[0][0]                  
==================================================================================================
Total params: 11,192,458
Trainable params: 11,184,650
Non-trainable params: 7,808
__________________________________________________________________________________________________
```
